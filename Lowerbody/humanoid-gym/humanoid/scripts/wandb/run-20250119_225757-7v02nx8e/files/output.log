[34m[1mwandb[0m: [33mWARNING[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/alexhuge/Documents/GitHub/Alexbotmini_gait/Lowerbody/humanoid-gym/logs/alexbotmini/Jan19_22-57-54_
Traceback (most recent call last):
  File "train.py", line 43, in <module>
    train(args)
  File "train.py", line 39, in train
    ppo_runner.learn(num_learning_iterations=train_cfg.runner.max_iterations, init_at_random_ep_len=True)
  File "/home/alexhuge/Documents/GitHub/Alexbotmini_gait/Lowerbody/humanoid-gym/humanoid/algo/ppo/on_policy_runner.py", line 163, in learn
    mean_value_loss, mean_surrogate_loss = self.alg.update()
  File "/home/alexhuge/Documents/GitHub/Alexbotmini_gait/Lowerbody/humanoid-gym/humanoid/algo/ppo/ppo.py", line 130, in update
    value_batch = self.actor_critic.evaluate(critic_obs_batch, masks=masks_batch, hidden_states=hid_states_batch[1])
  File "/home/alexhuge/Documents/GitHub/Alexbotmini_gait/Lowerbody/humanoid-gym/humanoid/algo/ppo/actor_critic.py", line 128, in evaluate
    value = self.critic(critic_observations)
  File "/home/alexhuge/anaconda3/envs/alexbotmini/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/alexhuge/anaconda3/envs/alexbotmini/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/alexhuge/anaconda3/envs/alexbotmini/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/alexhuge/anaconda3/envs/alexbotmini/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 516, in forward
    return F.elu(input, self.alpha, self.inplace)
  File "/home/alexhuge/anaconda3/envs/alexbotmini/lib/python3.8/site-packages/torch/nn/functional.py", line 1548, in elu
    result = torch._C._nn.elu(input, alpha)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 7.78 GiB total capacity; 450.22 MiB already allocated; 10.50 MiB free; 474.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
